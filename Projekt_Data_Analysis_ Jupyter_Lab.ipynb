{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                        comcast cable internet speeds\n",
      "1         payment disappear - service got disconnected\n",
      "2                                    speed and service\n",
      "3    comcast imposed a new usage cap of 300gb that ...\n",
      "4           comcast not working and no service to boot\n",
      "Name: Customer Complaint, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Inhalt der CSV 'Comcast.csv' lesen\n",
    "df = pd.read_csv ('Comcast.csv')\n",
    "# Inhalt der Spalte 'Customer Complaint in Kleinbuchstaben umwandeln \n",
    "df['Customer Complaint'] = df['Customer Complaint'].str.lower()\n",
    "print (df['Customer Complaint'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                   [comcast, cable, internet, speeds]\n",
      "1    [payment, disappear, -, service, got, disconne...\n",
      "2                                [speed, and, service]\n",
      "3    [comcast, imposed, a, new, usage, cap, of, 300...\n",
      "4    [comcast, not, working, and, no, service, to, ...\n",
      "Name: Customer Complaint, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Wörter mithilfe der Funktion word-tokenize aus der Unterbibiliothek nltk.tokenize tokenisiert.\n",
    "df['Customer Complaint'] = df['Customer Complaint'].apply(word_tokenize)\n",
    "print (df['Customer Complaint'].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eStopWords mit den englischen Stoppwörtern füllen\n",
    "eStopWords = set(stopwords.words('english'))\n",
    "#Comcast als zusätzliches Stopwort\n",
    "eStopWords.add('comcast')\n",
    "# Stoppwörter entfernen\n",
    "df['Customer Complaint'] = df['Customer Complaint'].apply(lambda x: [word for word in x if word not in eStopWords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                             [cable, internet, speed]\n",
      "1    [payment, disappear, -, service, got, disconne...\n",
      "2                                     [speed, service]\n",
      "3    [imposed, new, usage, cap, 300gb, punishes, st...\n",
      "4                             [working, service, boot]\n",
      "Name: Customer Complaint, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Wörter in die Grundform bringen (Stemming, Lemmatisierung) \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['Customer Complaint'] = df['Customer Complaint'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "print (df['Customer Complaint'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-Words-Vektorisierers\n",
    "bow_vectorizer = CountVectorizer()\n",
    "X_bow = bow_vectorizer.fit_transform(df['Customer Complaint'].apply(' '.join))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Ansatz. Beide Ansätze können in Python mithilfe des scikit-learn-Pakets umgesetzt dafür wird pandas als pd importiert.\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(df['Customer Complaint'].apply(' '.join))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSA\n",
    "lsa = TruncatedSVD(n_components=4, algorithm='randomized', n_iter=15, random_state=42)\n",
    "lsa_output = lsa.fit_transform(X)\n",
    "# Neue Spalte für jede Komponente im Data frame\n",
    "for i in range(lsa_output.shape[1]):\n",
    "    df[f'LSA Topic {i}'] = lsa_output[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "lda = LatentDirichletAllocation(n_components=3, doc_topic_prior=0.9, topic_word_prior=0.9)\n",
    "lda_output = lda.fit_transform(X)\n",
    "# Neue Spalte für jede Komponente im Data frame\n",
    "for i in range(lda_output.shape[1]):\n",
    "    df[f'LDA Topic {i}'] = lda_output[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verzeichnis für die Themen erstellen\n",
    "dictionary = Dictionary(df['Customer Complaint'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Umwandlung in eine vektorisierte Form durch Berechnung des \"Frequency counts\"\n",
    "corpus = [dictionary.doc2bow(doc) for doc in df['Customer Complaint']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['data', 'cap', 'internet'], ['issue', 'fee', 'service'], ['service', 'internet', 'speed']]\n"
     ]
    }
   ],
   "source": [
    "# Themen extrahieren\n",
    "n_top_words = 3 \n",
    "topics = []\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "    top_features = [feature_names[i] for i in top_features_ind]\n",
    "    topics.append(top_features)\n",
    "\n",
    "print (topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coherence Score (Bestimmung der Anzahl von Themen)\n",
    "coherence_model_lda = CoherenceModel(topics=topics, texts=df['Customer Complaint'], dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Wörter für Thema 1: data, cap, internet\n",
      "Beste Wörter für Thema 2: issue, fee, service\n",
      "Beste Wörter für Thema 3: service, internet, speed\n"
     ]
    }
   ],
   "source": [
    "#LDA Themen ausgeben\n",
    "for i, topic in enumerate(topics):\n",
    "    print(f\"Beste Wörter für Thema {i+1}: {', '.join(topic)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score:  0.7719403828058999\n"
     ]
    }
   ],
   "source": [
    "# Berechnung des Coherence Scores für LSA mithilfe von c_v measure\n",
    "topics = [[feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]] for topic in lsa.components_]\n",
    "coherence_model_lsa = CoherenceModel(topics=topics, texts=df['Customer Complaint'], dictionary=dictionary, coherence='c_v')\n",
    "coherence_lsa = coherence_model_lsa.get_coherence()\n",
    "print('Coherence Score: ', coherence_lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Wörter für Thema 1: internet, service, billing\n",
      "Beste Wörter für Thema 2: cap, data, usage\n",
      "Beste Wörter für Thema 3: billing, issue, practice\n",
      "Beste Wörter für Thema 4: service, customer, poor\n"
     ]
    }
   ],
   "source": [
    "#LSA Themen ausgeben\n",
    "for i, topic in enumerate(topics):\n",
    "    print(f\"Beste Wörter für Thema {i+1}: {', '.join(topic)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datei für die Ausgabe der Ergebnisse erzeugen\n",
    "df.to_csv('Comcast_Ergebnisse.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

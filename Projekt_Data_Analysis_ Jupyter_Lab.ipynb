{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                        comcast cable internet speeds\n",
      "1         payment disappear - service got disconnected\n",
      "2                                    speed and service\n",
      "3    comcast imposed a new usage cap of 300gb that ...\n",
      "4           comcast not working and no service to boot\n",
      "Name: Customer Complaint, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Inhalt der CSV 'Comcast.csv' lesen\n",
    "df = pd.read_csv ('Comcast.csv')\n",
    "# Inhalt der Spalte 'Customer Complaint in Kleinbuchstaben umwandeln \n",
    "df['Customer Complaint'] = df['Customer Complaint'].str.lower()\n",
    "print (df['Customer Complaint'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                   [comcast, cable, internet, speeds]\n",
      "1    [payment, disappear, -, service, got, disconne...\n",
      "2                                [speed, and, service]\n",
      "3    [comcast, imposed, a, new, usage, cap, of, 300...\n",
      "4    [comcast, not, working, and, no, service, to, ...\n",
      "Name: Customer Complaint, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Wörter mithilfe der Funktion word-tokenize aus der Unterbibiliothek nltk.tokenize tokenisiert.\n",
    "df['Customer Complaint'] = df['Customer Complaint'].apply(word_tokenize)\n",
    "print (df['Customer Complaint'].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'then', \"isn't\", \"that'll\", 'she', \"weren't\", 'him', 'than', 'yours', 'o', 'wouldn', 'each', 'itself', 'don', 'was', 'our', 'myself', 'mustn', 'whom', 'yourself', 'when', 'again', \"she's\", 'here', 'ours', 'shouldn', 'had', 'doesn', \"hasn't\", \"you'd\", 'y', \"shouldn't\", 'below', 'weren', 'won', 'having', 'they', 'comcast', 'them', 'couldn', 're', 'if', \"you're\", 'he', 'were', 'there', 'haven', 'more', 'will', 'my', \"mustn't\", 'how', 'from', 'further', 'this', 'm', 'have', 'hasn', 'which', 'is', 'who', 'both', 'hadn', 'on', \"you'll\", 'ourselves', \"you've\", 'theirs', 'that', 'shan', 'most', 'themselves', 'such', 'its', 'yourselves', 'of', 'while', 'aren', 'isn', 'their', 'being', 'against', 'am', 'be', 'by', 'some', 'between', 'd', 'ma', 'needn', \"couldn't\", \"hadn't\", 'own', 'ain', 'at', 'not', 'and', 'until', \"mightn't\", 'the', 'other', 'an', 'didn', 'been', \"should've\", 'me', 'only', 'with', 'after', 'should', 'we', 'those', 'because', 'what', 'over', 'her', 'about', 't', 'down', 'i', 'into', 'off', 'under', 'did', 'but', 'through', 'too', \"shan't\", 'during', 'it', \"it's\", 'herself', 'so', 'to', 'out', 'you', 'your', 'same', \"doesn't\", 'these', 'do', 'few', 'as', \"haven't\", 'his', 'where', 'or', 've', 'll', 'just', 'before', 'can', \"aren't\", 'wasn', 's', \"don't\", 'a', 'hers', \"needn't\", 'in', 'nor', 'himself', 'any', 'above', 'now', 'doing', \"wasn't\", \"didn't\", 'does', 'up', 'has', \"won't\", 'are', 'very', \"wouldn't\", 'all', 'no', 'once', 'for', 'mightn', 'why'}\n"
     ]
    }
   ],
   "source": [
    "# eStopWords mit den englischen Stoppwörtern füllen\n",
    "eStopWords = set(stopwords.words('english'))\n",
    "#Comcast als zusätzliches Stopwort\n",
    "eStopWords.add('comcast')\n",
    "# Stoppwörter entfernen\n",
    "df['Customer Complaint'] = df['Customer Complaint'].apply(lambda x: [word for word in x if word not in eStopWords])\n",
    "print (eStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                             [cable, internet, speed]\n",
      "1    [payment, disappear, -, service, got, disconne...\n",
      "2                                     [speed, service]\n",
      "3    [imposed, new, usage, cap, 300gb, punishes, st...\n",
      "4                             [working, service, boot]\n",
      "Name: Customer Complaint, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Wörter in die Grundform bringen (Stemming, Lemmatisierung) \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['Customer Complaint'] = df['Customer Complaint'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "print (df['Customer Complaint'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 201)\t1\n",
      "  (0, 671)\t1\n",
      "  (0, 1177)\t1\n",
      "  (1, 907)\t1\n",
      "  (1, 388)\t1\n",
      "  (1, 1132)\t1\n",
      "  (1, 563)\t1\n",
      "  (1, 394)\t1\n",
      "  (2, 1132)\t1\n",
      "  (2, 1176)\t1\n",
      "  (3, 618)\t1\n",
      "  (3, 828)\t1\n",
      "  (3, 1336)\t1\n",
      "  (3, 213)\t1\n",
      "  (3, 34)\t1\n",
      "  (3, 983)\t1\n",
      "  (3, 1192)\t1\n",
      "  (4, 1132)\t1\n",
      "  (4, 1393)\t1\n",
      "  (4, 181)\t1\n"
     ]
    }
   ],
   "source": [
    "# Bag-of-Words-Vektorisierers\n",
    "bow_vectorizer = CountVectorizer()\n",
    "X_bow = bow_vectorizer.fit_transform(df['Customer Complaint'].apply(' '.join))\n",
    "print(X_bow[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1177)\t0.6430858133577498\n",
      "  (0, 671)\t0.35391408383463274\n",
      "  (0, 201)\t0.6791063671631227\n",
      "  (1, 394)\t0.4603374037517078\n",
      "  (1, 563)\t0.518022386486237\n",
      "  (1, 1132)\t0.17857763377991173\n",
      "  (1, 388)\t0.5456269029782118\n",
      "  (1, 907)\t0.43605457601797254\n",
      "  (2, 1176)\t0.8300146883261219\n",
      "  (2, 1132)\t0.5577415325783895\n",
      "  (3, 1192)\t0.4100980454274462\n",
      "  (3, 983)\t0.4752447865207828\n",
      "  (3, 34)\t0.36424856605761413\n",
      "  (3, 213)\t0.239253169628378\n",
      "  (3, 1336)\t0.2880374421376605\n",
      "  (3, 828)\t0.3598540285809728\n",
      "  (3, 618)\t0.4512010627314486\n",
      "  (4, 181)\t0.7609589602524502\n",
      "  (4, 1393)\t0.5990941919907783\n",
      "  (4, 1132)\t0.2490534278712626\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Ansatz. Beide Ansätze können in Python mithilfe des scikit-learn-Pakets umgesetzt dafür wird pandas als pd importiert.\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(df['Customer Complaint'].apply(' '.join))\n",
    "print(X_tfidf[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.3317082  -0.1990938  -0.04354409]\n",
      " [ 0.08763926  0.0201744  -0.01508059]\n",
      " [ 0.39800402 -0.02014424 -0.07211096]\n",
      " ...\n",
      " [ 0.07726578  0.04768923 -0.00412558]\n",
      " [ 0.03602734  0.01963266 -0.00580637]\n",
      " [ 0.20847585 -0.12443873 -0.0312382 ]]\n"
     ]
    }
   ],
   "source": [
    "# LSA\n",
    "lsa = TruncatedSVD(n_components=3, algorithm='randomized', n_iter=15, random_state=42)\n",
    "lsa_output = lsa.fit_transform(X_tfidf)\n",
    "# Neue Spalte für jede Komponente im Data frame\n",
    "for i in range(lsa_output.shape[1]):\n",
    "    df[f'LSA Topic {i}'] = lsa_output[:, i]\n",
    "print (lsa_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.24948728 0.53977452 0.2107382 ]\n",
      " [0.29020813 0.43831662 0.27147525]\n",
      " [0.52328637 0.2477159  0.22899772]\n",
      " ...\n",
      " [0.51069816 0.24466928 0.24463256]\n",
      " [0.24568816 0.5082064  0.24610544]\n",
      " [0.51496735 0.24529266 0.23973999]]\n"
     ]
    }
   ],
   "source": [
    "# LDA\n",
    "lda = LatentDirichletAllocation(n_components=3, doc_topic_prior=0.9, topic_word_prior=0.9)\n",
    "lda_output = lda.fit_transform(X_tfidf)\n",
    "# Neue Spalte für jede Komponente im Data frame\n",
    "for i in range(lda_output.shape[1]):\n",
    "    df[f'LDA Topic {i}'] = lda_output[:, i]\n",
    "print (lda_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verzeichnis für die Themen erstellen\n",
    "dictionary = Dictionary(df['Customer Complaint'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Umwandlung in eine vektorisierte Form durch Berechnung des \"Frequency counts\"\n",
    "corpus = [dictionary.doc2bow(doc) for doc in df['Customer Complaint']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['internet', 'service', 'speed'], ['billing', 'service', 'internet'], ['data', 'cap', 'caps']]\n"
     ]
    }
   ],
   "source": [
    "# Themen extrahieren\n",
    "n_top_words = 3 \n",
    "topics = []\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "    top_features = [feature_names[i] for i in top_features_ind]\n",
    "    topics.append(top_features)\n",
    "\n",
    "print (topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coherence Score (Bestimmung der Anzahl von Themen)\n",
    "coherence_model_lda = CoherenceModel(topics=topics, texts=df['Customer Complaint'], dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Wörter für Thema 1: internet, service, billing\n",
      "Beste Wörter für Thema 2: billing, issues, practices\n",
      "Beste Wörter für Thema 3: data, cap, caps\n",
      "Beste Wörter für Thema 4: service, customer, poor\n"
     ]
    }
   ],
   "source": [
    "#LDA Themen ausgeben\n",
    "for i, topic in enumerate(topics):\n",
    "    print(f\"Beste Wörter für Thema {i+1}: {', '.join(topic)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score:  0.7028139925670622\n"
     ]
    }
   ],
   "source": [
    "# Berechnung des Coherence Scores für LSA mithilfe von c_v measure\n",
    "topics = [[feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]] for topic in lsa.components_]\n",
    "coherence_model_lsa = CoherenceModel(topics=topics, texts=df['Customer Complaint'], dictionary=dictionary, coherence='c_v')\n",
    "coherence_lsa = coherence_model_lsa.get_coherence()\n",
    "print('Coherence Score: ', coherence_lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Wörter für Thema 1: internet, service, billing\n",
      "Beste Wörter für Thema 2: billing, issues, practices\n",
      "Beste Wörter für Thema 3: data, cap, caps\n",
      "Beste Wörter für Thema 4: service, customer, poor\n"
     ]
    }
   ],
   "source": [
    "#LSA Themen ausgeben\n",
    "for i, topic in enumerate(topics):\n",
    "    print(f\"Beste Wörter für Thema {i+1}: {', '.join(topic)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datei für die Ausgabe der Ergebnisse erzeugen\n",
    "df.to_csv('Comcast_Ergebnisse.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
